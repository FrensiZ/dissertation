{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Any, ClassVar, Dict, List, Optional, Type, TypeVar, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.buffers import RolloutBuffer\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance, get_schedule_fn, obs_as_tensor\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "\n",
    "from sb3_contrib.common.recurrent.buffers import RecurrentDictRolloutBuffer, RecurrentRolloutBuffer\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "from sb3_contrib.common.recurrent.type_aliases import RNNStates\n",
    "from sb3_contrib.ppo_recurrent.policies import CnnLstmPolicy, MlpLstmPolicy, MultiInputLstmPolicy\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.distributions import Distribution\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.torch_layers import (\n",
    "    BaseFeaturesExtractor,\n",
    "    CombinedExtractor,\n",
    "    FlattenExtractor,\n",
    "    MlpExtractor,\n",
    "    NatureCNN,\n",
    ")\n",
    "from stable_baselines3.common.type_aliases import Schedule\n",
    "from stable_baselines3.common.utils import zip_strict\n",
    "from torch import nn\n",
    "\n",
    "from sb3_contrib.common.recurrent.type_aliases import RNNStates\n",
    "import pandas as pd\n",
    "\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelfRecurrentPPO= TypeVar(\"SelfRecurrentPPO\", bound=\"RecurrentPPO\")\n",
    "\n",
    "\n",
    "class RecurrentPPO_custom(OnPolicyAlgorithm):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (PPO) (clip version)\n",
    "    with support for recurrent policies (LSTM).\n",
    "\n",
    "    Based on the original Stable Baselines 3 implementation.\n",
    "\n",
    "    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
    "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
    "    :param learning_rate: The learning rate, it can be a function\n",
    "        of the current progress remaining (from 1 to 0)\n",
    "    :param n_steps: The number of steps to run for each environment per update\n",
    "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "    :param batch_size: Minibatch size\n",
    "    :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
    "    :param gamma: Discount factor\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "    :param clip_range: Clipping parameter, it can be a function of the current progress\n",
    "        remaining (from 1 to 0).\n",
    "    :param clip_range_vf: Clipping parameter for the value function,\n",
    "        it can be a function of the current progress remaining (from 1 to 0).\n",
    "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
    "        no clipping will be done on the value function.\n",
    "        IMPORTANT: this clipping depends on the reward scaling.\n",
    "    :param normalize_advantage: Whether to normalize or not the advantage\n",
    "    :param ent_coef: Entropy coefficient for the loss calculation\n",
    "    :param vf_coef: Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: The maximum value for the gradient clipping\n",
    "    :param target_kl: Limit the KL divergence between updates,\n",
    "        because the clipping is not enough to prevent large update\n",
    "        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
    "        By default, there is no limit on the kl div.\n",
    "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
    "        the reported success rate, mean episode length, and mean reward over\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
    "    :param verbose: the verbosity level: 0 no output, 1 info, 2 debug\n",
    "    :param seed: Seed for the pseudo random generators\n",
    "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
    "        Setting it to auto, the code will be run on the GPU if possible.\n",
    "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
    "    \"\"\"\n",
    "\n",
    "    policy_aliases: ClassVar[Dict[str, Type[BasePolicy]]] = {\n",
    "        \"MlpLstmPolicy\": MlpLstmPolicy,\n",
    "        \"CnnLstmPolicy\": CnnLstmPolicy,\n",
    "        \"MultiInputLstmPolicy\": MultiInputLstmPolicy,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, Type[RecurrentActorCriticPolicy]],\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule] = 3e-4,\n",
    "        n_steps: int = 128,\n",
    "        batch_size: Optional[int] = 128,\n",
    "        n_epochs: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_range: Union[float, Schedule] = 0.2,\n",
    "        clip_range_vf: Union[None, float, Schedule] = None,\n",
    "        normalize_advantage: bool = True,\n",
    "        ent_coef: float = 0.0,\n",
    "        vf_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        use_sde: bool = False,\n",
    "        sde_sample_freq: int = -1,\n",
    "        target_kl: Optional[float] = None,\n",
    "        stats_window_size: int = 100,\n",
    "        tensorboard_log: Optional[str] = None,\n",
    "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            stats_window_size=stats_window_size,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            seed=seed,\n",
    "            device=device,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.clip_range = clip_range\n",
    "        self.clip_range_vf = clip_range_vf\n",
    "        self.normalize_advantage = normalize_advantage\n",
    "        self.target_kl = target_kl\n",
    "        self._last_lstm_states = None\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        self._setup_lr_schedule()\n",
    "        self.set_random_seed(self.seed)\n",
    "\n",
    "        buffer_cls = RecurrentDictRolloutBuffer if isinstance(self.observation_space, spaces.Dict) else RecurrentRolloutBuffer\n",
    "\n",
    "        self.policy = self.policy_class(\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            self.lr_schedule,\n",
    "            use_sde=self.use_sde,\n",
    "            **self.policy_kwargs,\n",
    "        )\n",
    "        self.policy = self.policy.to(self.device)\n",
    "\n",
    "        # We assume that LSTM for the actor and the critic\n",
    "        # have the same architecture\n",
    "        lstm = self.policy.lstm_actor\n",
    "\n",
    "        if not isinstance(self.policy, RecurrentActorCriticPolicy):\n",
    "            raise ValueError(\"Policy must subclass RecurrentActorCriticPolicy\")\n",
    "\n",
    "        single_hidden_state_shape = (lstm.num_layers, self.n_envs, lstm.hidden_size)\n",
    "        # hidden and cell states for actor and critic\n",
    "        self._last_lstm_states = RNNStates(\n",
    "            (\n",
    "                th.zeros(single_hidden_state_shape, device=self.device),\n",
    "                th.zeros(single_hidden_state_shape, device=self.device),\n",
    "            ),\n",
    "            (\n",
    "                th.zeros(single_hidden_state_shape, device=self.device),\n",
    "                th.zeros(single_hidden_state_shape, device=self.device),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        hidden_state_buffer_shape = (self.n_steps, lstm.num_layers, self.n_envs, lstm.hidden_size)\n",
    "\n",
    "        self.rollout_buffer = buffer_cls(\n",
    "            self.n_steps,\n",
    "            self.observation_space,\n",
    "            self.action_space,\n",
    "            hidden_state_buffer_shape,\n",
    "            self.device,\n",
    "            gamma=self.gamma,\n",
    "            gae_lambda=self.gae_lambda,\n",
    "            n_envs=self.n_envs,\n",
    "        )\n",
    "\n",
    "        # Initialize schedules for policy/value clipping\n",
    "        self.clip_range = get_schedule_fn(self.clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            if isinstance(self.clip_range_vf, (float, int)):\n",
    "                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive, pass `None` to deactivate vf clipping\"\n",
    "\n",
    "            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n",
    "\n",
    "    def collect_rollouts(\n",
    "        self,\n",
    "        env: VecEnv,\n",
    "        callback: BaseCallback,\n",
    "        rollout_buffer: RolloutBuffer,\n",
    "        n_rollout_steps: int,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Collect experiences using the current policy and fill a ``RolloutBuffer``.\n",
    "        The term rollout here refers to the model-free notion and should not\n",
    "        be used with the concept of rollout used in model-based RL or planning.\n",
    "\n",
    "        :param env: The training environment\n",
    "        :param callback: Callback that will be called at each step\n",
    "            (and at the beginning and end of the rollout)\n",
    "        :param rollout_buffer: Buffer to fill with rollouts\n",
    "        :param n_steps: Number of experiences to collect per environment\n",
    "        :return: True if function returned with at least `n_rollout_steps`\n",
    "            collected, False if callback terminated rollout prematurely.\n",
    "        \"\"\"\n",
    "\n",
    "        print('ROLLOUT START')\n",
    "\n",
    "        assert isinstance(\n",
    "            rollout_buffer, (RecurrentRolloutBuffer, RecurrentDictRolloutBuffer)\n",
    "        ), f\"{rollout_buffer} doesn't support recurrent policy\"\n",
    "\n",
    "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
    "        # Switch to eval mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(False)\n",
    "\n",
    "        n_steps = 0\n",
    "        rollout_buffer.reset()\n",
    "        # Sample new weights for the state dependent exploration\n",
    "        if self.use_sde:\n",
    "            self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "        callback.on_rollout_start()\n",
    "\n",
    "        lstm_states = deepcopy(self._last_lstm_states)\n",
    "\n",
    "        while n_steps < n_rollout_steps:\n",
    "            if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n",
    "                # Sample a new noise matrix\n",
    "                self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "            with th.no_grad():\n",
    "                # Convert to pytorch tensor or to TensorDict\n",
    "                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
    "                episode_starts = th.tensor(self._last_episode_starts, dtype=th.float32, device=self.device)\n",
    "                actions, values, log_probs, lstm_states = self.policy.forward(obs_tensor, lstm_states, episode_starts)\n",
    "\n",
    "            actions = actions.cpu().numpy()\n",
    "\n",
    "            # Rescale and perform action\n",
    "            clipped_actions = actions\n",
    "            # Clip the actions to avoid out of bound error\n",
    "            if isinstance(self.action_space, spaces.Box):\n",
    "                clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "\n",
    "            new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
    "\n",
    "            self.num_timesteps += env.num_envs\n",
    "\n",
    "            # Give access to local variables\n",
    "            callback.update_locals(locals())\n",
    "            if not callback.on_step():\n",
    "                return False\n",
    "\n",
    "            self._update_info_buffer(infos, dones)\n",
    "            n_steps += 1\n",
    "\n",
    "            if isinstance(self.action_space, spaces.Discrete):\n",
    "                # Reshape in case of discrete action\n",
    "                actions = actions.reshape(-1, 1)\n",
    "\n",
    "            # Handle timeout by bootstraping with value function\n",
    "            # see GitHub issue #633\n",
    "            for idx, done_ in enumerate(dones):\n",
    "                if (\n",
    "                    done_\n",
    "                    and infos[idx].get(\"terminal_observation\") is not None\n",
    "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
    "                ):\n",
    "                    terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
    "                    with th.no_grad():\n",
    "                        terminal_lstm_state = (\n",
    "                            lstm_states.vf[0][:, idx : idx + 1, :].contiguous(),\n",
    "                            lstm_states.vf[1][:, idx : idx + 1, :].contiguous(),\n",
    "                        )\n",
    "                        # terminal_lstm_state = None\n",
    "                        episode_starts = th.tensor([False], dtype=th.float32, device=self.device)\n",
    "                        terminal_value = self.policy.predict_values(terminal_obs, terminal_lstm_state, episode_starts)[0]\n",
    "                    rewards[idx] += self.gamma * terminal_value\n",
    "\n",
    "            rollout_buffer.add(\n",
    "                self._last_obs,\n",
    "                actions,\n",
    "                rewards,\n",
    "                self._last_episode_starts,\n",
    "                values,\n",
    "                log_probs,\n",
    "                lstm_states=self._last_lstm_states,\n",
    "            )\n",
    "\n",
    "            self._last_obs = new_obs\n",
    "            self._last_episode_starts = dones\n",
    "            self._last_lstm_states = lstm_states\n",
    "\n",
    "        with th.no_grad():\n",
    "            # Compute value for the last timestep\n",
    "            episode_starts = th.tensor(dones, dtype=th.float32, device=self.device)\n",
    "            values = self.policy.predict_values(obs_as_tensor(new_obs, self.device), lstm_states.vf, episode_starts)\n",
    "\n",
    "        rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "\n",
    "        callback.on_rollout_end()\n",
    "        \n",
    "\n",
    "        #############\n",
    "        # ===========\n",
    "        # print('ROLLOUT END')\n",
    "\n",
    "        # for i, (obs, action, reward, ep_start, value) in enumerate(zip(\n",
    "        #     rollout_buffer.observations, \n",
    "        #     rollout_buffer.actions, \n",
    "        #     rollout_buffer.rewards, \n",
    "        #     rollout_buffer.episode_starts,\n",
    "        #     rollout_buffer.values\n",
    "        # )):\n",
    "        #     print(f\"Step {i}: Observation: {obs}, Action: {action}, Reward: {reward}, Episode Start: {ep_start}, Values: {value}\")\n",
    "        #     if ep_start:\n",
    "        #         print(\"---- New Sequence Start ----\")\n",
    "        \n",
    "        # print(\"\")\n",
    "        # print(\"Rollout Buffer Inspection Completed.\")\n",
    "        # print(\"\")\n",
    "        # print(\"Observation Size: \", rollout_buffer.observations.shape)\n",
    "        # print(\"Actions Size: \", rollout_buffer.actions.shape)\n",
    "        # print(\"Rewards Size: \", rollout_buffer.rewards.shape)\n",
    "        # print(\"Episodes Size: \", rollout_buffer.episode_starts.shape)\n",
    "        # print(\"\")\n",
    "        # print('LSTM SIZE: ', rollout_buffer.hidden_states_pi.shape)\n",
    "\n",
    "        # print(\"\")\n",
    "\n",
    "        # print('LSTM Hidden PI: ', rollout_buffer.hidden_states_pi[1][0][0][:10])\n",
    "        # print('LSTM Cell PI: ', rollout_buffer.cell_states_pi[1][0][0][:10])\n",
    "        # print(\"\")\n",
    "        # print('LSTM Hidden VF: ', rollout_buffer.hidden_states_vf[1][0][0][:10])\n",
    "        # print('LSTM Cell VF: ', rollout_buffer.cell_states_vf[1][0][0][:10])\n",
    "\n",
    "\n",
    "        # ===========\n",
    "        #############\n",
    "\n",
    "        return True\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Update policy using the currently gathered rollout buffer.\n",
    "        \"\"\"\n",
    "\n",
    "        print('TRAIN START')\n",
    "\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update optimizer learning rate\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "        # Compute current clip range\n",
    "        clip_range = self.clip_range(self._current_progress_remaining)\n",
    "        # Optional: clip range for the value function\n",
    "        if self.clip_range_vf is not None:\n",
    "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)\n",
    "\n",
    "        entropy_losses = []\n",
    "        pg_losses, value_losses = [], []\n",
    "        clip_fractions = []\n",
    "\n",
    "        continue_training = True\n",
    "\n",
    "        # train for n_epochs epochs\n",
    "        for epoch in range(self.n_epochs):\n",
    "            approx_kl_divs = []\n",
    "            # Do a complete pass on the rollout buffer\n",
    "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
    "                \n",
    "\n",
    "                actions = rollout_data.actions\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Convert discrete action from float to long\n",
    "                    actions = rollout_data.actions.long().flatten()\n",
    "\n",
    "                # Convert mask from float to bool\n",
    "                mask = rollout_data.mask > 1e-8\n",
    "\n",
    "                print('OBS', rollout_data.observations.shape)\n",
    "                print('ACT', actions.shape)\n",
    "                print('Episode Starts', rollout_data.episode_starts.shape)\n",
    "\n",
    "                values, log_prob, entropy = self.policy.evaluate_actions(\n",
    "                    rollout_data.observations,\n",
    "                    actions,\n",
    "                    rollout_data.lstm_states,\n",
    "                    rollout_data.episode_starts,\n",
    "                )\n",
    "\n",
    "                print('values', values.shape)\n",
    "                print('log_prob', log_prob.shape)\n",
    "                print('Entropy', entropy.shape)\n",
    "                print('')\n",
    "                # print('values', values)\n",
    "                # print('log_prob', log_prob)\n",
    "                # print('Entropy', entropy)\n",
    "\n",
    "                values = values.flatten()\n",
    "\n",
    "                print('')\n",
    "                print('values', values.shape)\n",
    "                # Normalize advantage\n",
    "                advantages = rollout_data.advantages\n",
    "                if self.normalize_advantage:\n",
    "                    advantages = (advantages - advantages[mask].mean()) / (advantages[mask].std() + 1e-8)\n",
    "\n",
    "                # ratio between old and new policy, should be one at the first iteration\n",
    "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
    "\n",
    "                # clipped surrogate loss\n",
    "                policy_loss_1 = advantages * ratio\n",
    "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "                policy_loss = -th.mean(th.min(policy_loss_1, policy_loss_2)[mask])\n",
    "\n",
    "                # Logging\n",
    "                pg_losses.append(policy_loss.item())\n",
    "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()[mask]).item()\n",
    "                clip_fractions.append(clip_fraction)\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    # No clipping\n",
    "                    values_pred = values\n",
    "                else:\n",
    "                    # Clip the different between old and new value\n",
    "                    # NOTE: this depends on the reward scaling\n",
    "                    values_pred = rollout_data.old_values + th.clamp(\n",
    "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
    "                    )\n",
    "                # Value loss using the TD(gae_lambda) target\n",
    "                # Mask padded sequences\n",
    "                value_loss = th.mean(((rollout_data.returns - values_pred) ** 2)[mask])\n",
    "\n",
    "                value_losses.append(value_loss.item())\n",
    "\n",
    "                # Entropy loss favor exploration\n",
    "                if entropy is None:\n",
    "                    # Approximate entropy when no analytical form\n",
    "                    entropy_loss = -th.mean(-log_prob[mask])\n",
    "                else:\n",
    "                    entropy_loss = -th.mean(entropy[mask])\n",
    "\n",
    "                entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "\n",
    "                # Calculate approximate form of reverse KL Divergence for early stopping\n",
    "                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\n",
    "                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\n",
    "                # and Schulman blog: http://joschu.net/blog/kl-approx.html\n",
    "                with th.no_grad():\n",
    "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
    "                    approx_kl_div = th.mean(((th.exp(log_ratio) - 1) - log_ratio)[mask]).cpu().numpy()\n",
    "                    approx_kl_divs.append(approx_kl_div)\n",
    "\n",
    "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
    "                    continue_training = False\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
    "                    break\n",
    "\n",
    "                # Optimization step\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Clip grad norm\n",
    "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "\n",
    "            if not continue_training:\n",
    "                break\n",
    "\n",
    "        self._n_updates += self.n_epochs\n",
    "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
    "\n",
    "        # Logs\n",
    "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
    "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
    "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
    "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
    "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
    "        self.logger.record(\"train/loss\", loss.item())\n",
    "        self.logger.record(\"train/explained_variance\", explained_var)\n",
    "        if hasattr(self.policy, \"log_std\"):\n",
    "            self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/clip_range\", clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
    "\n",
    "        print('TRAIN END')\n",
    "\n",
    "    def learn(\n",
    "        self: SelfRecurrentPPO,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 1,\n",
    "        tb_log_name: str = \"RecurrentPPO\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ) -> SelfRecurrentPPO:\n",
    "        return super().learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=callback,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=reset_num_timesteps,\n",
    "            progress_bar=progress_bar,\n",
    "        )\n",
    "\n",
    "    def _excluded_save_params(self) -> List[str]:\n",
    "        return super()._excluded_save_params() + [\"_last_lstm_states\"]  # noqa: RUF005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentActorCriticPolicy_custom(RecurrentActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Recurrent policy class for actor-critic algorithms (has both policy and value prediction).\n",
    "    To be used with A2C, PPO and the likes.\n",
    "    It assumes that both the actor and the critic LSTM\n",
    "    have the same architecture.\n",
    "\n",
    "    :param observation_space: Observation space\n",
    "    :param action_space: Action space\n",
    "    :param lr_schedule: Learning rate schedule (could be constant)\n",
    "    :param net_arch: The specification of the policy and value networks.\n",
    "    :param activation_fn: Activation function\n",
    "    :param ortho_init: Whether to use or not orthogonal initialization\n",
    "    :param use_sde: Whether to use State Dependent Exploration or not\n",
    "    :param log_std_init: Initial value for the log standard deviation\n",
    "    :param full_std: Whether to use (n_features x n_actions) parameters\n",
    "        for the std instead of only (n_features,) when using gSDE\n",
    "    :param use_expln: Use ``expln()`` function instead of ``exp()`` to ensure\n",
    "        a positive standard deviation (cf paper). It allows to keep variance\n",
    "        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n",
    "    :param squash_output: Whether to squash the output using a tanh function,\n",
    "        this allows to ensure boundaries when using gSDE.\n",
    "    :param features_extractor_class: Features extractor to use.\n",
    "    :param features_extractor_kwargs: Keyword arguments\n",
    "        to pass to the features extractor.\n",
    "    :param share_features_extractor: If True, the features extractor is shared between the policy and value networks.\n",
    "    :param normalize_images: Whether to normalize images or not,\n",
    "         dividing by 255.0 (True by default)\n",
    "    :param optimizer_class: The optimizer to use,\n",
    "        ``th.optim.Adam`` by default\n",
    "    :param optimizer_kwargs: Additional keyword arguments,\n",
    "        excluding the learning rate, to pass to the optimizer\n",
    "    :param lstm_hidden_size: Number of hidden units for each LSTM layer.\n",
    "    :param n_lstm_layers: Number of LSTM layers.\n",
    "    :param shared_lstm: Whether the LSTM is shared between the actor and the critic\n",
    "        (in that case, only the actor gradient is used)\n",
    "        By default, the actor and the critic have two separate LSTM.\n",
    "    :param enable_critic_lstm: Use a seperate LSTM for the critic.\n",
    "    :param lstm_kwargs: Additional keyword arguments to pass the the LSTM\n",
    "        constructor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Schedule,\n",
    "        net_arch: Optional[Union[List[int], Dict[str, List[int]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        ortho_init: bool = True,\n",
    "        use_sde: bool = False,\n",
    "        log_std_init: float = 0.0,\n",
    "        full_std: bool = True,\n",
    "        use_expln: bool = False,\n",
    "        squash_output: bool = False,\n",
    "        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n",
    "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        share_features_extractor: bool = True,\n",
    "        normalize_images: bool = True,\n",
    "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
    "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        lstm_hidden_size: int = 256,\n",
    "        n_lstm_layers: int = 1,\n",
    "        shared_lstm: bool = False,\n",
    "        enable_critic_lstm: bool = True,\n",
    "        lstm_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        self.lstm_output_dim = lstm_hidden_size\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            ortho_init,\n",
    "            use_sde,\n",
    "            log_std_init,\n",
    "            full_std,\n",
    "            use_expln,\n",
    "            squash_output,\n",
    "            features_extractor_class,\n",
    "            features_extractor_kwargs,\n",
    "            share_features_extractor,\n",
    "            normalize_images,\n",
    "            optimizer_class,\n",
    "            optimizer_kwargs,\n",
    "        )\n",
    "\n",
    "        self.lstm_kwargs = lstm_kwargs or {}\n",
    "        self.shared_lstm = shared_lstm\n",
    "        self.enable_critic_lstm = enable_critic_lstm\n",
    "        self.lstm_actor = nn.LSTM(\n",
    "            self.features_dim,\n",
    "            lstm_hidden_size,\n",
    "            num_layers=n_lstm_layers,\n",
    "            **self.lstm_kwargs,\n",
    "        )\n",
    "        # For the predict() method, to initialize hidden states\n",
    "        # (n_lstm_layers, batch_size, lstm_hidden_size)\n",
    "        self.lstm_hidden_state_shape = (n_lstm_layers, 1, lstm_hidden_size)\n",
    "        self.critic = None\n",
    "        self.lstm_critic = None\n",
    "        assert not (\n",
    "            self.shared_lstm and self.enable_critic_lstm\n",
    "        ), \"You must choose between shared LSTM, seperate or no LSTM for the critic.\"\n",
    "\n",
    "        assert not (\n",
    "            self.shared_lstm and not self.share_features_extractor\n",
    "        ), \"If the features extractor is not shared, the LSTM cannot be shared.\"\n",
    "\n",
    "        # No LSTM for the critic, we still need to convert\n",
    "        # output of features extractor to the correct size\n",
    "        # (size of the output of the actor lstm)\n",
    "        if not (self.shared_lstm or self.enable_critic_lstm):\n",
    "            self.critic = nn.Linear(self.features_dim, lstm_hidden_size)\n",
    "\n",
    "        # Use a separate LSTM for the critic\n",
    "        if self.enable_critic_lstm:\n",
    "            self.lstm_critic = nn.LSTM(\n",
    "                self.features_dim,\n",
    "                lstm_hidden_size,\n",
    "                num_layers=n_lstm_layers,\n",
    "                **self.lstm_kwargs,\n",
    "            )\n",
    "\n",
    "        # Setup optimizer with initial learning rate\n",
    "        self.optimizer = self.optimizer_class(self.parameters(), lr=lr_schedule(1), **self.optimizer_kwargs)\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        \"\"\"\n",
    "        Create the policy and value networks.\n",
    "        Part of the layers can be shared.\n",
    "        \"\"\"\n",
    "        self.mlp_extractor = MlpExtractor(\n",
    "            self.lstm_output_dim,\n",
    "            net_arch=self.net_arch,\n",
    "            activation_fn=self.activation_fn,\n",
    "            device=self.device,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _process_sequence(\n",
    "        features: th.Tensor,\n",
    "        lstm_states: Tuple[th.Tensor, th.Tensor],\n",
    "        episode_starts: th.Tensor,\n",
    "        lstm: nn.LSTM,\n",
    "    ) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Do a forward pass in the LSTM network.\n",
    "\n",
    "        :param features: Input tensor\n",
    "        :param lstm_states: previous hidden and cell states of the LSTM, respectively\n",
    "        :param episode_starts: Indicates when a new episode starts,\n",
    "            in that case, we need to reset LSTM states.\n",
    "        :param lstm: LSTM object.\n",
    "        :return: LSTM output and updated LSTM states.\n",
    "        \"\"\"\n",
    "\n",
    "        # LSTM logic\n",
    "        # (sequence length, batch size, features dim)\n",
    "        # (batch size = n_envs for data collection or n_seq when doing gradient update)\n",
    "        n_seq = lstm_states[0].shape[1]\n",
    "        # Batch to sequence\n",
    "        # (padded batch size, features_dim) -> (n_seq, max length, features_dim) -> (max length, n_seq, features_dim)\n",
    "        # note: max length (max sequence length) is always 1 during data collection\n",
    "        features_sequence = features.reshape((n_seq, -1, lstm.input_size)).swapaxes(0, 1)\n",
    "        episode_starts = episode_starts.reshape((n_seq, -1)).swapaxes(0, 1)\n",
    "\n",
    "        # If we don't have to reset the state in the middle of a sequence\n",
    "        # we can avoid the for loop, which speeds up things\n",
    "        if th.all(episode_starts == 0.0):\n",
    "            lstm_output, lstm_states = lstm(features_sequence, lstm_states)\n",
    "            lstm_output = th.flatten(lstm_output.transpose(0, 1), start_dim=0, end_dim=1)\n",
    "            return lstm_output, lstm_states\n",
    "\n",
    "        lstm_output = []\n",
    "        # Iterate over the sequence\n",
    "        for features, episode_start in zip_strict(features_sequence, episode_starts):\n",
    "            hidden, lstm_states = lstm(\n",
    "                features.unsqueeze(dim=0),\n",
    "                (\n",
    "                    # Reset the states at the beginning of a new episode\n",
    "                    (1.0 - episode_start).view(1, n_seq, 1) * lstm_states[0],\n",
    "                    (1.0 - episode_start).view(1, n_seq, 1) * lstm_states[1],\n",
    "                ),\n",
    "            )\n",
    "            lstm_output += [hidden]\n",
    "        # Sequence to batch\n",
    "        # (sequence length, n_seq, lstm_out_dim) -> (batch_size, lstm_out_dim)\n",
    "        lstm_output = th.flatten(th.cat(lstm_output).transpose(0, 1), start_dim=0, end_dim=1)\n",
    "        return lstm_output, lstm_states\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        obs: th.Tensor,\n",
    "        lstm_states: RNNStates,\n",
    "        episode_starts: th.Tensor,\n",
    "        deterministic: bool = False,\n",
    "    ) -> Tuple[th.Tensor, th.Tensor, th.Tensor, RNNStates]:\n",
    "        \"\"\"\n",
    "        Forward pass in all the networks (actor and critic)\n",
    "\n",
    "        :param obs: Observation. Observation\n",
    "        :param lstm_states: The last hidden and memory states for the LSTM.\n",
    "        :param episode_starts: Whether the observations correspond to new episodes\n",
    "            or not (we reset the lstm states in that case).\n",
    "        :param deterministic: Whether to sample or use deterministic actions\n",
    "        :return: action, value and log probability of the action\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        if self.share_features_extractor:\n",
    "            pi_features = vf_features = features  # alis\n",
    "        else:\n",
    "            pi_features, vf_features = features\n",
    "        # latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        latent_pi, lstm_states_pi = self._process_sequence(pi_features, lstm_states.pi, episode_starts, self.lstm_actor)\n",
    "        if self.lstm_critic is not None:\n",
    "            latent_vf, lstm_states_vf = self._process_sequence(vf_features, lstm_states.vf, episode_starts, self.lstm_critic)\n",
    "        elif self.shared_lstm:\n",
    "            # Re-use LSTM features but do not backpropagate\n",
    "            latent_vf = latent_pi.detach()\n",
    "            lstm_states_vf = (lstm_states_pi[0].detach(), lstm_states_pi[1].detach())\n",
    "        else:\n",
    "            # Critic only has a feedforward network\n",
    "            latent_vf = self.critic(vf_features)\n",
    "            lstm_states_vf = lstm_states_pi\n",
    "\n",
    "        latent_pi = self.mlp_extractor.forward_actor(latent_pi)\n",
    "        latent_vf = self.mlp_extractor.forward_critic(latent_vf)\n",
    "\n",
    "        # Evaluate the values for the given observations\n",
    "        values = self.value_net(latent_vf)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        actions = distribution.get_actions(deterministic=deterministic)\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        return actions, values, log_prob, RNNStates(lstm_states_pi, lstm_states_vf)\n",
    "\n",
    "    def get_distribution(\n",
    "        self,\n",
    "        obs: th.Tensor,\n",
    "        lstm_states: Tuple[th.Tensor, th.Tensor],\n",
    "        episode_starts: th.Tensor,\n",
    "    ) -> Tuple[Distribution, Tuple[th.Tensor, ...]]:\n",
    "        \"\"\"\n",
    "        Get the current policy distribution given the observations.\n",
    "\n",
    "        :param obs: Observation.\n",
    "        :param lstm_states: The last hidden and memory states for the LSTM.\n",
    "        :param episode_starts: Whether the observations correspond to new episodes\n",
    "            or not (we reset the lstm states in that case).\n",
    "        :return: the action distribution and new hidden states.\n",
    "        \"\"\"\n",
    "        # Call the method from the parent of the parent class\n",
    "        features = super(ActorCriticPolicy, self).extract_features(obs, self.pi_features_extractor)\n",
    "        latent_pi, lstm_states = self._process_sequence(features, lstm_states, episode_starts, self.lstm_actor)\n",
    "        latent_pi = self.mlp_extractor.forward_actor(latent_pi)\n",
    "        return self._get_action_dist_from_latent(latent_pi), lstm_states\n",
    "\n",
    "    def predict_values(\n",
    "        self,\n",
    "        obs: th.Tensor,\n",
    "        lstm_states: Tuple[th.Tensor, th.Tensor],\n",
    "        episode_starts: th.Tensor,\n",
    "    ) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Get the estimated values according to the current policy given the observations.\n",
    "\n",
    "        :param obs: Observation.\n",
    "        :param lstm_states: The last hidden and memory states for the LSTM.\n",
    "        :param episode_starts: Whether the observations correspond to new episodes\n",
    "            or not (we reset the lstm states in that case).\n",
    "        :return: the estimated values.\n",
    "        \"\"\"\n",
    "        # Call the method from the parent of the parent class\n",
    "        features = super(ActorCriticPolicy, self).extract_features(obs, self.vf_features_extractor)\n",
    "\n",
    "        if self.lstm_critic is not None:\n",
    "            latent_vf, lstm_states_vf = self._process_sequence(features, lstm_states, episode_starts, self.lstm_critic)\n",
    "        elif self.shared_lstm:\n",
    "            # Use LSTM from the actor\n",
    "            latent_pi, _ = self._process_sequence(features, lstm_states, episode_starts, self.lstm_actor)\n",
    "            latent_vf = latent_pi.detach()\n",
    "        else:\n",
    "            latent_vf = self.critic(features)\n",
    "\n",
    "        latent_vf = self.mlp_extractor.forward_critic(latent_vf)\n",
    "        return self.value_net(latent_vf)\n",
    "\n",
    "\n",
    "#########\n",
    "#########\n",
    "#########\n",
    "#########\n",
    "    def evaluate_actions(\n",
    "        self, obs: th.Tensor, actions: th.Tensor, lstm_states: RNNStates, episode_starts: th.Tensor\n",
    "    ) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "\n",
    "        :param obs: Observation.\n",
    "        :param actions:\n",
    "        :param lstm_states: The last hidden and memory states for the LSTM.\n",
    "        :param episode_starts: Whether the observations correspond to new episodes\n",
    "            or not (we reset the lstm states in that case).\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            and entropy of the action distribution.\n",
    "        \"\"\"\n",
    "        print('Evaluate Function')\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        if self.share_features_extractor:\n",
    "            print('share_features_extractor')\n",
    "            pi_features = vf_features = features  # alias\n",
    "        else:\n",
    "            print('else')\n",
    "            pi_features, vf_features = features\n",
    "        latent_pi, _ = self._process_sequence(pi_features, lstm_states.pi, episode_starts, self.lstm_actor)\n",
    "        if self.lstm_critic is not None:\n",
    "            print('lstm_critic')\n",
    "            latent_vf, _ = self._process_sequence(vf_features, lstm_states.vf, episode_starts, self.lstm_critic)\n",
    "        elif self.shared_lstm:\n",
    "            print('shared_lstm')\n",
    "            latent_vf = latent_pi.detach()\n",
    "        else:\n",
    "            print('second else')\n",
    "            latent_vf = self.critic(vf_features)\n",
    "\n",
    "        latent_pi = self.mlp_extractor.forward_actor(latent_pi)\n",
    "        latent_vf = self.mlp_extractor.forward_critic(latent_vf)\n",
    "\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "\n",
    "        return values, log_prob, distribution.entropy()\n",
    "\n",
    "    def _predict(\n",
    "        self,\n",
    "        observation: th.Tensor,\n",
    "        lstm_states: Tuple[th.Tensor, th.Tensor],\n",
    "        episode_starts: th.Tensor,\n",
    "        deterministic: bool = False,\n",
    "    ) -> Tuple[th.Tensor, Tuple[th.Tensor, ...]]:\n",
    "        \"\"\"\n",
    "        Get the action according to the policy for a given observation.\n",
    "\n",
    "        :param observation:\n",
    "        :param lstm_states: The last hidden and memory states for the LSTM.\n",
    "        :param episode_starts: Whether the observations correspond to new episodes\n",
    "            or not (we reset the lstm states in that case).\n",
    "        :param deterministic: Whether to use stochastic or deterministic actions\n",
    "        :return: Taken action according to the policy and hidden states of the RNN\n",
    "        \"\"\"\n",
    "        distribution, lstm_states = self.get_distribution(observation, lstm_states, episode_starts)\n",
    "        return distribution.get_actions(deterministic=deterministic), lstm_states\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        observation: Union[np.ndarray, Dict[str, np.ndarray]],\n",
    "        state: Optional[Tuple[np.ndarray, ...]] = None,\n",
    "        episode_start: Optional[np.ndarray] = None,\n",
    "        deterministic: bool = False,\n",
    "    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:\n",
    "        \"\"\"\n",
    "        Get the policy action from an observation (and optional hidden state).\n",
    "        Includes sugar-coating to handle different observations (e.g. normalizing images).\n",
    "\n",
    "        :param observation: the input observation\n",
    "        :param lstm_states: The last hidden and memory states for the LSTM.\n",
    "        :param episode_starts: Whether the observations correspond to new episodes\n",
    "            or not (we reset the lstm states in that case).\n",
    "        :param deterministic: Whether or not to return deterministic actions.\n",
    "        :return: the model's action and the next hidden state\n",
    "            (used in recurrent policies)\n",
    "        \"\"\"\n",
    "        # Switch to eval mode (this affects batch norm / dropout)\n",
    "        self.set_training_mode(False)\n",
    "\n",
    "        observation, vectorized_env = self.obs_to_tensor(observation)\n",
    "\n",
    "        if isinstance(observation, dict):\n",
    "            n_envs = observation[next(iter(observation.keys()))].shape[0]\n",
    "        else:\n",
    "            n_envs = observation.shape[0]\n",
    "        # state : (n_layers, n_envs, dim)\n",
    "        if state is None:\n",
    "            # Initialize hidden states to zeros\n",
    "            state = np.concatenate([np.zeros(self.lstm_hidden_state_shape) for _ in range(n_envs)], axis=1)\n",
    "            state = (state, state)\n",
    "\n",
    "        if episode_start is None:\n",
    "            episode_start = np.array([False for _ in range(n_envs)])\n",
    "\n",
    "        with th.no_grad():\n",
    "            # Convert to PyTorch tensors\n",
    "            states = th.tensor(state[0], dtype=th.float32, device=self.device), th.tensor(\n",
    "                state[1], dtype=th.float32, device=self.device\n",
    "            )\n",
    "            episode_starts = th.tensor(episode_start, dtype=th.float32, device=self.device)\n",
    "            actions, states = self._predict(\n",
    "                observation, lstm_states=states, episode_starts=episode_starts, deterministic=deterministic\n",
    "            )\n",
    "            states = (states[0].cpu().numpy(), states[1].cpu().numpy())\n",
    "\n",
    "        # Convert to numpy\n",
    "        actions = actions.cpu().numpy()\n",
    "\n",
    "        if isinstance(self.action_space, spaces.Box):\n",
    "            if self.squash_output:\n",
    "                # Rescale to proper domain when using squashing\n",
    "                actions = self.unscale_action(actions)\n",
    "            else:\n",
    "                # Actions could be on arbitrary scale, so clip the actions to avoid\n",
    "                # out of bound error (e.g. if sampling from a Gaussian distribution)\n",
    "                actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "\n",
    "        # Remove batch dimension if needed\n",
    "        if not vectorized_env:\n",
    "            actions = actions.squeeze(axis=0)\n",
    "\n",
    "        return actions, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "ROLLOUT START\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.9     |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 3034     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 300      |\n",
      "---------------------------------\n",
      "TRAIN START\n",
      "OBS torch.Size([204, 4])\n",
      "ACT torch.Size([204])\n",
      "Episode Starts torch.Size([204])\n",
      "Evaluate Function\n",
      "share_features_extractor\n",
      "lstm_critic\n",
      "values torch.Size([204, 1])\n",
      "log_prob torch.Size([204])\n",
      "Entropy torch.Size([204])\n",
      "\n",
      "\n",
      "values torch.Size([204])\n",
      "OBS torch.Size([198, 4])\n",
      "ACT torch.Size([198])\n",
      "Episode Starts torch.Size([198])\n",
      "Evaluate Function\n",
      "share_features_extractor\n",
      "lstm_critic\n",
      "values torch.Size([198, 1])\n",
      "log_prob torch.Size([198])\n",
      "Entropy torch.Size([198])\n",
      "\n",
      "\n",
      "values torch.Size([198])\n",
      "OBS torch.Size([60, 4])\n",
      "ACT torch.Size([60])\n",
      "Episode Starts torch.Size([60])\n",
      "Evaluate Function\n",
      "share_features_extractor\n",
      "lstm_critic\n",
      "values torch.Size([60, 1])\n",
      "log_prob torch.Size([60])\n",
      "Entropy torch.Size([60])\n",
      "\n",
      "\n",
      "values torch.Size([60])\n",
      "TRAIN END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RecurrentPPO_custom at 0x16f7d0430>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_model = RecurrentPPO_custom(policy=RecurrentActorCriticPolicy_custom, \n",
    "                                    env=\"CartPole-v1\", \n",
    "                                    verbose=1,\n",
    "                                    n_steps=300,\n",
    "                                    n_epochs=1)\n",
    "default_model.learn(total_timesteps=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
