{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from flax import linen as nn\n",
    "import gymnax\n",
    "import optax\n",
    "from gymnax.environments import environment, spaces\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Constant stock price')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# constant price\n",
    "data = jnp.array([jnp.arange(500), jnp.repeat(100,500)])\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(data[0],data[1])\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Constant stock price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingSimulator:\n",
    "    \n",
    "    def __init__(self, trading_days, training):\n",
    "        self.trading_days = jnp.int32(trading_days)\n",
    "        \n",
    "        self.initial_state = {\n",
    "            'counter': jnp.int32(0),\n",
    "            'done': jnp.bool_(False),\n",
    "            'training': jnp.bool_(training),\n",
    "            'states': jnp.array([100.], dtype=jnp.float32),\n",
    "            'actions': jnp.array([0], dtype=jnp.int32),\n",
    "            'rewards': jnp.array([0.], dtype=jnp.float32)\n",
    "        }\n",
    "    \n",
    "    def take_step(self, simulator_state, action):\n",
    "        curr_action = action\n",
    "        last_action = simulator_state['actions'][-1]\n",
    "\n",
    "        # REWARD\n",
    "        reward = jnp.where(curr_action != last_action, 0.05, -0.05)\n",
    "        rewards = jnp.concatenate([simulator_state['rewards'], jnp.array([reward])])\n",
    "\n",
    "        # Update stock price based on action\n",
    "        last_state = simulator_state['states'][-1]\n",
    "        new_state = jnp.where(curr_action == 1, last_state + 5.0, last_state - 5.0)\n",
    "        states = jnp.concatenate([simulator_state['states'], jnp.array([new_state])])\n",
    "\n",
    "        # Update actions\n",
    "        actions = jnp.concatenate([simulator_state['actions'], jnp.array([curr_action])])\n",
    "\n",
    "        # Increment counter\n",
    "        counter = simulator_state['counter'] + 1\n",
    "        done = counter >= self.trading_days\n",
    "\n",
    "        # Update simulator state\n",
    "        simulator_state = {\n",
    "            'counter': counter,\n",
    "            'done': done,\n",
    "            'training': simulator_state['training'],\n",
    "            'states': states,\n",
    "            'actions': actions,\n",
    "            'rewards': rewards\n",
    "        }\n",
    "\n",
    "        info = {'reward': reward}\n",
    "\n",
    "        return simulator_state, reward, info\n",
    "    \n",
    "    def reset(self, training):\n",
    "        new_state = self.initial_state.copy()\n",
    "        new_state['training'] = jnp.bool_(training)\n",
    "        return new_state\n",
    "        \n",
    "    def result(self, simulator_state):\n",
    "        result = {\n",
    "            'states': simulator_state['states'],\n",
    "            'actions': simulator_state['actions'],\n",
    "            'rewards': simulator_state['rewards']\n",
    "        }\n",
    "        return result\n",
    "\n",
    "class TradingEnvironment(environment.Environment):\n",
    "    \n",
    "    def __init__(self, trading_days, training):\n",
    "        self.trading_days = jnp.int32(trading_days)\n",
    "        self.training = jnp.bool_(training)\n",
    "        self.num_features = 1\n",
    "\n",
    "        # Initialize the simulator\n",
    "        self.simulator = TradingSimulator(trading_days=self.trading_days, training=self.training)\n",
    "        \n",
    "        # Action Space (0,1)\n",
    "        self._action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Observation Space\n",
    "        self._observation_space = spaces.Box(\n",
    "            low=-jnp.inf, \n",
    "            high=jnp.inf, \n",
    "            shape=(self.num_features,), \n",
    "            dtype=jnp.float32\n",
    "        )\n",
    "    \n",
    "    def step(self, env_state, action):\n",
    "       # Perform a step using the simulator's state\n",
    "       env_state, reward, info = self.simulator.take_step(env_state, action)\n",
    "       # Extract observation (last price) from the env_state\n",
    "       observation = env_state['states'][-1]\n",
    "       return observation, reward, env_state, info\n",
    "    \n",
    "    def reset(self, training):\n",
    "        \n",
    "        # RNG_KEY will be not used in the beginning\n",
    "          \n",
    "        # Reset the simulator and return the initial state\n",
    "        env_state = self.simulator.reset(training)\n",
    "        # Extract the initial observation (last price) for the first step\n",
    "        observation = env_state['states'][-1]\n",
    "        return observation, env_state\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._action_space\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._observation_space\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "a = jnp.array([1,2,3,4.])\n",
    "\n",
    "a = jnp.concatenate([a,jnp.array([a[-1] + 1])])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.]\n",
      "95.0\n",
      "-0.05\n",
      "[100.  95.]\n",
      "90.0\n",
      "-0.05\n",
      "[100.  95.  90.]\n",
      "95.0\n",
      "0.05\n",
      "[100.  95.  90.  95.]\n"
     ]
    }
   ],
   "source": [
    "abc = TradingEnvironment(trading_days=252, training=False)\n",
    "simulator_state = abc.simulator.initial_state\n",
    "print(simulator_state['states'])\n",
    "observation, reward, simulator_state, info = abc.step(env_state=simulator_state, action=0)\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(simulator_state['states'])\n",
    "observation, reward, simulator_state, info = abc.step(env_state=simulator_state, action=0)\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(simulator_state['states'])\n",
    "observation, reward, simulator_state, info = abc.step(env_state=simulator_state, action=1)\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(simulator_state['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.]\n",
      "\n",
      "[100.  95.]\n",
      "-0.05\n",
      "False\n",
      "{'reward': -0.05}\n",
      "[ 0.   -0.05]\n",
      "\n",
      "[100.  95.  90.]\n",
      "-0.05\n",
      "False\n",
      "{'reward': -0.05}\n",
      "[ 0.   -0.05 -0.05]\n",
      "\n",
      "[100.  95.  90.  95.]\n",
      "0.05\n",
      "False\n",
      "{'reward': 0.05}\n",
      "[ 0.   -0.05 -0.05  0.05]\n",
      "[100.] False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(abc.simulator.status()[0])\n",
    "print(\"\")\n",
    "observation, reward, done, info = abc.step(action=0)\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)\n",
    "print(abc.simulator.rewards)\n",
    "print(\"\")\n",
    "observation, reward, done, info = abc.step(action=0)\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)\n",
    "print(abc.simulator.rewards)\n",
    "print(\"\")\n",
    "observation, reward, done, info = abc.step(action=1)\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)\n",
    "print(abc.simulator.rewards)\n",
    "\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "obs, done = abc.reset(rng_key, training=False)\n",
    "print(obs, done)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy/Value Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    # Number of possible actions\n",
    "    n_actions: int  \n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # A simple feedforward network\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        # Output logits for each action\n",
    "        logits = nn.Dense(self.n_actions)(x)  \n",
    "        # Raw logits are returned; softmax can be applied later\n",
    "        return logits\n",
    "    \n",
    "class ValueNetwork(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        value = nn.Dense(1)(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(s,a,\\theta_k,\\theta) = min(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}*A(s,a), clip(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}, 1 - \\epsilon, 1 + \\epsilon)*A(s,a))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_clipped_objective(logits, old_logits, actions, advantages, epsilon=0.2):\n",
    "    \n",
    "    \"\"\"Compute PPO clipped objective.\n",
    "    Args:\n",
    "        logits: Logits from the policy network (new policy).\n",
    "        old_logits: Logits from the old policy.\n",
    "        actions: Actions taken by the agent.\n",
    "        advantages: Advantage estimates.\n",
    "        epsilon: Clipping parameter.\n",
    "    Returns:\n",
    "        Clipped surrogate objective.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the probabilities from logits\n",
    "    probs = nn.softmax(logits)\n",
    "    old_probs = nn.softmax(old_logits)\n",
    "    \n",
    "    # Select the probabilities of the actions taken\n",
    "    probs_act = jnp.take_along_axis(probs, actions[:, None], axis=-1).squeeze()\n",
    "    old_probs_act = jnp.take_along_axis(old_probs, actions[:, None], axis=-1).squeeze()\n",
    "    \n",
    "    # Ratio of new to old policy\n",
    "    ratios = probs_act / old_probs_act\n",
    "    \n",
    "    # Clipped objective\n",
    "    clipped_ratios = jnp.clip(ratios, 1.0 - epsilon, 1.0 + epsilon)\n",
    "    loss = -jnp.mean(jnp.minimum(ratios * advantages, clipped_ratios * advantages))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function_loss(predicted_values, target_values):\n",
    "    \"\"\"Compute value function loss (MSE).\n",
    "    Args:\n",
    "        predicted_values: Predicted value estimates from the value network.\n",
    "        target_values: Target values (returns).\n",
    "    Returns:\n",
    "        Mean squared error loss for the value function.\n",
    "    \"\"\"\n",
    "    return jnp.mean((predicted_values - target_values) ** 2)\n",
    "\n",
    "def entropy_bonus(logits):\n",
    "    \"\"\"Compute entropy bonus.\n",
    "    Args:\n",
    "        logits: Logits from the policy network.\n",
    "    Returns:\n",
    "        Entropy bonus term.\n",
    "    \"\"\"\n",
    "    probs = nn.softmax(logits)\n",
    "    entropy = -jnp.sum(probs * jnp.log(probs + 1e-10), axis=-1)\n",
    "    return jnp.mean(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: [[ 0.02306527 -0.5690973 ]\n",
      " [ 0.02306527 -0.5690973 ]\n",
      " [ 0.02306527 -0.5690973 ]\n",
      " [ 0.02306527 -0.5690973 ]\n",
      " [ 0.02306527 -0.5690973 ]\n",
      " [ 0.02306527 -0.5690973 ]\n",
      " [ 0.02306527 -0.5690973 ]\n",
      " [ 0.02306527 -0.5690973 ]\n",
      " [ 0.02306527 -0.5690973 ]\n",
      " [ 0.02306527 -0.5690973 ]]\n",
      "Value: [[-0.64239705]\n",
      " [-0.64239705]\n",
      " [-0.64239705]\n",
      " [-0.64239705]\n",
      " [-0.64239705]\n",
      " [-0.64239705]\n",
      " [-0.64239705]\n",
      " [-0.64239705]\n",
      " [-0.64239705]\n",
      " [-0.64239705]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize networks\n",
    "key = random.PRNGKey(0)\n",
    "policy_network = PolicyNetwork(n_actions=2)\n",
    "value_network = ValueNetwork()\n",
    "\n",
    "# Dummy input (representing a state)\n",
    "dummy_state = jnp.ones((10, 1))  # Assuming input dimension is 1\n",
    "\n",
    "# Initialize parameters\n",
    "policy_params = policy_network.init(key, dummy_state)\n",
    "value_params = value_network.init(key, dummy_state)\n",
    "\n",
    "# Forward pass to test\n",
    "logits = policy_network.apply(policy_params, dummy_state)\n",
    "value = value_network.apply(value_params, dummy_state)\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Value:\", value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GAE (Generalised Advantages Estimation):**\n",
    "\n",
    "$$\n",
    "A_t = \\sum_{i=0}^{\\infty} (\\gamma \\lambda)^i \\delta_{t+i}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\gamma$ is the discount factor.\n",
    "- $\\lambda$ parameter that controls the bias-variance trade-off.\n",
    "- $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.99, lambda_=0.95):\n",
    "    \"\"\"Compute Generalized Advantage Estimation (GAE) and normalize the advantages.\n",
    "    Args:\n",
    "        rewards: Array of rewards from the environment.\n",
    "        values: Array of value estimates from the value network.\n",
    "        dones: Array indicating episode termination.\n",
    "        gamma: Discount factor.\n",
    "        lambda_: GAE parameter.\n",
    "    Returns:\n",
    "        Normalized advantages (A_t) and discounted returns (R_t).\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    values = jnp.append(values, 0)  # Append 0 for the last value estimate (bootstrap)\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lambda_ * gae * (1 - dones[t])\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    advantages = jnp.array(advantages)\n",
    "    returns = advantages + values[:-1]  # Calculate returns (R_t)\n",
    "    \n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - jnp.mean(advantages)) / (jnp.std(advantages) + 1e-8)\n",
    "    \n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Advantages: [ 1.212037    0.02503157 -1.2370689 ]\n",
      "Returns: [2.8730676 1.96525   1.       ]\n"
     ]
    }
   ],
   "source": [
    "# Dummy rewards, values, and dones arrays for testing\n",
    "rewards = jnp.array([1.0, 1.0, 1.0])\n",
    "values = jnp.array([0.5, 0.5, 0.5])\n",
    "dones = jnp.array([0, 0, 1])\n",
    "\n",
    "# Compute normalized GAE and returns\n",
    "normalized_advantages, returns = compute_gae(rewards, values, dones)\n",
    "\n",
    "print(\"Normalized Advantages:\", normalized_advantages)\n",
    "print(\"Returns:\", returns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trajectory Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(env, env_state, policy_params, value_params, key, num_steps, observation):\n",
    "    \"\"\"Collect trajectories by interacting with the environment.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment instance.\n",
    "        env_state: The current state of the environment.\n",
    "        policy_params: Parameters of the policy network.\n",
    "        value_params: Parameters of the value network.\n",
    "        key: RNG key for sampling actions.\n",
    "        num_steps: Number of steps to collect.\n",
    "        observation: Initial observation from the environment.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the collected trajectories.\n",
    "    \"\"\"\n",
    "    trajectories = {\n",
    "        'states': [],\n",
    "        'actions': [],\n",
    "        'rewards': [],\n",
    "        'values': [],\n",
    "        'dones': [],\n",
    "        'next_states': []\n",
    "    }\n",
    "\n",
    "    observation = jnp.array(observation).reshape(-1, 1) \n",
    "\n",
    "    for i in range(num_steps):\n",
    "        print(i)\n",
    "        # Sample action from the policy\n",
    "        logits = policy_network.apply(policy_params, observation)\n",
    "        action_probabilities = nn.softmax(logits)\n",
    "        key, subkey = random.split(key)\n",
    "        print(jnp.arange(2).shape)\n",
    "        print(action_probabilities)\n",
    "        print(action_probabilities[0])\n",
    "        print(action_probabilities[0][0])\n",
    "        print(action_probabilities.shape)\n",
    "        action = random.choice(subkey, a=jnp.arange(2), p=action_probabilities[0])\n",
    "        \n",
    "        # Get value estimate for the current state\n",
    "        value = value_network.apply(value_params, jnp.array([observation]))\n",
    "\n",
    "        # Step in the environment\n",
    "        next_observation, reward, env_state, _ = env.step(env_state, action)\n",
    "\n",
    "        # Store trajectory data\n",
    "        trajectories['states'].append(observation)\n",
    "        trajectories['actions'].append(action)\n",
    "        trajectories['rewards'].append(reward)\n",
    "        trajectories['values'].append(value)\n",
    "        trajectories['dones'].append(env_state['done'])\n",
    "        trajectories['next_states'].append(next_observation)\n",
    "\n",
    "        # Update observation\n",
    "        observation = next_observation\n",
    "\n",
    "        if env_state['done']:\n",
    "            break\n",
    "\n",
    "    # Convert lists to JAX arrays\n",
    "    trajectories = {k: jnp.array(v) for k, v in trajectories.items()}\n",
    "    \n",
    "    return trajectories, env_state, key\n",
    "\n",
    "\n",
    "def process_trajectories(trajectories):\n",
    "    \"\"\"Compute returns and advantages for the collected trajectories.\"\"\"\n",
    "    rewards = trajectories['rewards']\n",
    "    values = trajectories['values']\n",
    "    dones = trajectories['dones']\n",
    "    \n",
    "    # Compute GAE advantages and returns\n",
    "    advantages, returns = compute_gae(rewards, values, dones)\n",
    "    \n",
    "    # Add returns and advantages to trajectories\n",
    "    trajectories['advantages'] = advantages\n",
    "    trajectories['returns'] = returns\n",
    "    \n",
    "    return trajectories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Policy and Value Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_and_value_networks(trajectories, policy_params, value_params, policy_opt_state, value_opt_state):\n",
    "    \"\"\"Update the policy and value networks using the collected trajectories.\"\"\"\n",
    "    \n",
    "    def loss_fn(policy_params, value_params):\n",
    "        logits = policy_network.apply(policy_params, trajectories['states'])\n",
    "        values = value_network.apply(value_params, trajectories['states']).squeeze()\n",
    "        \n",
    "        # Calculate losses\n",
    "        policy_loss = ppo_clipped_objective(logits, logits, trajectories['actions'], trajectories['advantages'])\n",
    "        value_loss = value_function_loss(values, trajectories['returns'])\n",
    "        entropy = entropy_bonus(logits)\n",
    "        \n",
    "        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "        \n",
    "        return total_loss, (policy_loss, value_loss, entropy)\n",
    "    \n",
    "    # Compute gradients and update the parameters\n",
    "    (loss, (policy_loss, value_loss, entropy)), grads = jax.value_and_grad(loss_fn, argnums=(0, 1), has_aux=True)(policy_params, value_params)\n",
    "    # Update parameters\n",
    "    updates_policy, policy_opt_state = policy_optimizer.update(grads[0], policy_opt_state)\n",
    "    updates_value, value_opt_state = value_optimizer.update(grads[1], value_opt_state)\n",
    "    \n",
    "    policy_params = optax.apply_updates(policy_params, updates_policy)\n",
    "    value_params = optax.apply_updates(value_params, updates_value)\n",
    "    \n",
    "    return policy_params, value_params, policy_opt_state, value_opt_state, loss, policy_loss, value_loss, entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batch Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(trajectories, policy_params, value_params, policy_opt_state, value_opt_state, key, num_epochs=10, batch_size=32):\n",
    "    \"\"\"Perform PPO updates using mini-batches.\"\"\"\n",
    "    num_samples = len(trajectories['states'])\n",
    "    indices = jnp.arange(num_samples)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        permuted_indices = random.permutation(subkey, indices)\n",
    "        \n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_indices = permuted_indices[start:end]\n",
    "            \n",
    "            batch = {k: v[batch_indices] for k, v in trajectories.items()}\n",
    "            \n",
    "            policy_params, value_params, policy_opt_state, value_opt_state, loss, policy_loss, value_loss, entropy = update_policy_and_value_networks(\n",
    "                batch, policy_params, value_params, policy_opt_state, value_opt_state\n",
    "            )\n",
    "    \n",
    "    return policy_params, value_params, policy_opt_state, value_opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and networks\n",
    "env = TradingEnvironment(trading_days=100, training=True)\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "# Initialize the networks\n",
    "policy_network = PolicyNetwork(n_actions=2)\n",
    "value_network = ValueNetwork()\n",
    "\n",
    "# Dummy input to initialize the network parameters\n",
    "dummy_state = jnp.ones((10, 1))  # Assuming input dimension is 1\n",
    "\n",
    "# Initialize parameters\n",
    "policy_params = policy_network.init(key, dummy_state)\n",
    "value_params = value_network.init(key, dummy_state)\n",
    "\n",
    "# Set up optimizer\n",
    "learning_rate = 3e-4\n",
    "policy_optimizer = optax.adam(learning_rate)\n",
    "value_optimizer = optax.adam(learning_rate)\n",
    "\n",
    "policy_opt_state = policy_optimizer.init(policy_params)\n",
    "value_opt_state = value_optimizer.init(value_params)\n",
    "\n",
    "# Reset the environment\n",
    "observation, env_state = env.reset(training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "[[1.0000000e+00 1.9173973e-26]]\n",
      "(2,)\n",
      "(1, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "p must be None or match the shape of a",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(jnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(action_probabilities\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_probabilities\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/jax/_src/random.py:661\u001b[0m, in \u001b[0;36mchoice\u001b[0;34m(key, a, shape, replace, p, axis)\u001b[0m\n\u001b[1;32m    659\u001b[0m p_arr, \u001b[38;5;241m=\u001b[39m promote_dtypes_inexact(p)\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p_arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (n_inputs,):\n\u001b[0;32m--> 661\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp must be None or match the shape of a\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m replace:\n\u001b[1;32m    663\u001b[0m   p_cuml \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mcumsum(p_arr)\n",
      "\u001b[0;31mValueError\u001b[0m: p must be None or match the shape of a"
     ]
    }
   ],
   "source": [
    "observation = jnp.array(observation).reshape(-1, 1)\n",
    "logits = policy_network.apply(policy_params, observation)\n",
    "print(logits.shape)\n",
    "action_probabilities = nn.softmax(logits)\n",
    "print(action_probabilities)\n",
    "key, subkey = random.split(key)\n",
    "print(jnp.arange(2).shape)\n",
    "print(action_probabilities.shape)\n",
    "random.choice(subkey, a=jnp.arange(2), p=action_probabilities)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(2,)\n",
      "[[1.0000000e+00 1.9173973e-26]]\n",
      "[1.0000000e+00 1.9173973e-26]\n",
      "1.0\n",
      "(1, 2)\n",
      "1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_updates \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m update \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_updates):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Collect trajectories\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     trajectories, env_state, key \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(trajectories[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(trajectories[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[197], line 30\u001b[0m, in \u001b[0;36mcollect_trajectories\u001b[0;34m(env, env_state, policy_params, value_params, key, num_steps, observation)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Sample action from the policy\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m action_probabilities \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39msoftmax(logits)\n\u001b[1;32m     32\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key)\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[148], line 9\u001b[0m, in \u001b[0;36mPolicyNetwork.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# A simple feedforward network\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m64\u001b[39m)(x)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/flax/linen/linear.py:259\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;129m@compact\u001b[39m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    The transformed input.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m   kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_init,\n\u001b[0;32m--> 259\u001b[0m     (\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures),\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype,\n\u001b[1;32m    261\u001b[0m   )\n\u001b[1;32m    262\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    263\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[1;32m    264\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_init, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype\n\u001b[1;32m    265\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "num_updates = 1000\n",
    "for update in range(num_updates):\n",
    "    # Collect trajectories\n",
    "    trajectories, env_state, key = collect_trajectories(env, env_state, policy_params, value_params, key, num_steps=10, observation=observation)\n",
    "    print(trajectories['states'].shape)\n",
    "    print(trajectories['actions'].shape)\n",
    "    print(trajectories['rewards'].shape)\n",
    "    # Compute returns and advantages\n",
    "    trajectories = process_trajectories(trajectories)\n",
    "    \n",
    "    # Update networks using PPO\n",
    "    policy_params, value_params, policy_opt_state, value_opt_state, key = ppo_update(\n",
    "        trajectories, policy_params, value_params, policy_opt_state, value_opt_state, key\n",
    "    )\n",
    "    \n",
    "    if update % 100 == 0:\n",
    "        print(f\"Update {update}: Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
